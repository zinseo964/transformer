{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA0vI5vUPNgDgmyTCWtXNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zinseo964/transformer/blob/main/medium_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24rYU2bhCwfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils.py"
      ],
      "metadata": {
        "id": "mTSQ8hTiCcmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch import nn, Tensor\n",
        "from typing import Optional, Any, Union, Callable, Tuple\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
        "    Modified from:\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "    Args:\n",
        "\n",
        "        dim1: int, for both src and tgt masking, this must be target sequence\n",
        "              length\n",
        "\n",
        "        dim2: int, for src masking this must be encoder sequence length (i.e.\n",
        "              the length of the input sequence to the model),\n",
        "              and for tgt masking, this must be target sequence length\n",
        "\n",
        "\n",
        "    Return:\n",
        "\n",
        "        A Tensor of shape [dim1, dim2]\n",
        "    \"\"\"\n",
        "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n",
        "\n",
        "\n",
        "def get_indices_input_target(num_obs, input_len, step_size, forecast_horizon, target_len):\n",
        "        \"\"\"\n",
        "        Produce all the start and end index positions of all sub-sequences.\n",
        "        The indices will be used to split the data into sub-sequences on which\n",
        "        the models will be trained.\n",
        "\n",
        "        Returns a tuple with four elements:\n",
        "        1) The index position of the first element to be included in the input sequence\n",
        "        2) The index position of the last element to be included in the input sequence\n",
        "        3) The index position of the first element to be included in the target sequence\n",
        "        4) The index position of the last element to be included in the target sequence\n",
        "\n",
        "\n",
        "        Args:\n",
        "            num_obs (int): Number of observations in the entire dataset for which\n",
        "                            indices must be generated.\n",
        "\n",
        "            input_len (int): Length of the input sequence (a sub-sequence of\n",
        "                             of the entire data sequence)\n",
        "\n",
        "            step_size (int): Size of each step as the data sequence is traversed.\n",
        "                             If 1, the first sub-sequence will be indices 0-input_len,\n",
        "                             and the next will be 1-input_len.\n",
        "\n",
        "            forecast_horizon (int): How many index positions is the target away from\n",
        "                                    the last index position of the input sequence?\n",
        "                                    If forecast_horizon=1, and the input sequence\n",
        "                                    is data[0:10], the target will be data[11:taget_len].\n",
        "\n",
        "            target_len (int): Length of the target / output sequence.\n",
        "        \"\"\"\n",
        "\n",
        "        input_len = round(input_len) # just a precaution\n",
        "        start_position = 0\n",
        "        stop_position = num_obs-1 # because of 0 indexing\n",
        "\n",
        "        subseq_first_idx = start_position\n",
        "        subseq_last_idx = start_position + input_len\n",
        "        target_first_idx = subseq_last_idx + forecast_horizon\n",
        "        target_last_idx = target_first_idx + target_len\n",
        "        print(\"target_last_idx is {}\".format(target_last_idx))\n",
        "        print(\"stop_position is {}\".format(stop_position))\n",
        "        indices = []\n",
        "        while target_last_idx <= stop_position:\n",
        "            indices.append((subseq_first_idx, subseq_last_idx, target_first_idx, target_last_idx))\n",
        "            subseq_first_idx += step_size\n",
        "            subseq_last_idx += step_size\n",
        "            target_first_idx = subseq_last_idx + forecast_horizon\n",
        "            target_last_idx = target_first_idx + target_len\n",
        "\n",
        "        return indices\n",
        "\n",
        "def get_indices_entire_sequence(data: pd.DataFrame, window_size: int, step_size: int) -> list:\n",
        "        \"\"\"\n",
        "        Produce all the start and end index positions that is needed to produce\n",
        "        the sub-sequences.\n",
        "\n",
        "        Returns a list of tuples. Each tuple is (start_idx, end_idx) of a sub-\n",
        "        sequence. These tuples should be used to slice the dataset into sub-\n",
        "        sequences. These sub-sequences should then be passed into a function\n",
        "        that slices them into input and target sequences.\n",
        "\n",
        "        Args:\n",
        "            num_obs (int): Number of observations (time steps) in the entire\n",
        "                           dataset for which indices must be generated, e.g.\n",
        "                           len(data)\n",
        "\n",
        "            window_size (int): The desired length of each sub-sequence. Should be\n",
        "                               (input_sequence_length + target_sequence_length)\n",
        "                               E.g. if you want the model to consider the past 100\n",
        "                               time steps in order to predict the future 50\n",
        "                               time steps, window_size = 100+50 = 150\n",
        "\n",
        "            step_size (int): Size of each step as the data sequence is traversed\n",
        "                             by the moving window.\n",
        "                             If 1, the first sub-sequence will be [0:window_size],\n",
        "                             and the next will be [1:window_size].\n",
        "\n",
        "        Return:\n",
        "            indices: a list of tuples\n",
        "        \"\"\"\n",
        "\n",
        "        stop_position = len(data)-1 # 1- because of 0 indexing\n",
        "\n",
        "        # Start the first sub-sequence at index position 0\n",
        "        subseq_first_idx = 0\n",
        "\n",
        "        subseq_last_idx = window_size\n",
        "\n",
        "        indices = []\n",
        "\n",
        "        while subseq_last_idx <= stop_position:\n",
        "\n",
        "            indices.append((subseq_first_idx, subseq_last_idx))\n",
        "\n",
        "            subseq_first_idx += step_size\n",
        "\n",
        "            subseq_last_idx += step_size\n",
        "\n",
        "        return indices\n",
        "\n",
        "# rain = pd.read_csv('/content/drive/MyDrive/240705/rain.csv', encoding = 'utf-8')\n",
        "\n",
        "def read_data(data_dir: Union[str, Path] = \"/content/drive/MyDrive/240705/medium\",\n",
        "    timestamp_col_name: str=\"timestamp\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read data from csv file and return pd.Dataframe object\n",
        "\n",
        "    Args:\n",
        "\n",
        "        data_dir: str or Path object specifying the path to the directory\n",
        "                  containing the data\n",
        "\n",
        "        target_col_name: str, the name of the column containing the target variable\n",
        "\n",
        "        timestamp_col_name: str, the name of the column or named index\n",
        "                            containing the timestamps\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure that `data_dir` is a Path object\n",
        "    data_dir = Path(data_dir)\n",
        "\n",
        "    # Read csv file\n",
        "    csv_files = list(data_dir.glob(\"*.csv\"))\n",
        "\n",
        "    if len(csv_files) > 1:\n",
        "        raise ValueError(\"data_dir contains more than 1 csv file. Must only contain 1\")\n",
        "    elif len(csv_files) == 0:\n",
        "\t    raise ValueError(\"data_dir must contain at least 1 csv file.\")\n",
        "\n",
        "    data_path = csv_files[0]\n",
        "\n",
        "    print(\"Reading file in {}\".format(data_path))\n",
        "\n",
        "    data = pd.read_csv(\n",
        "        data_path,\n",
        "        parse_dates=[timestamp_col_name],\n",
        "        index_col=[timestamp_col_name],\n",
        "        infer_datetime_format=True,\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "    # Make sure all \"n/e\" values have been removed from df.\n",
        "    if is_ne_in_df(data):\n",
        "        raise ValueError(\"data frame contains 'n/e' values. These must be handled\")\n",
        "\n",
        "    data = to_numeric_and_downcast_data(data)\n",
        "\n",
        "    # Make sure data is in ascending order by timestamp\n",
        "    data.sort_values(by=[timestamp_col_name], inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def is_ne_in_df(df:pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Some raw data files contain cells with \"n/e\". This function checks whether\n",
        "    any column in a df contains a cell with \"n/e\". Returns False if no columns\n",
        "    contain \"n/e\", True otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    for col in df.columns:\n",
        "\n",
        "        true_bool = (df[col] == \"n/e\")\n",
        "\n",
        "        if any(true_bool):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def to_numeric_and_downcast_data(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Downcast columns in df to smallest possible version of it's existing data\n",
        "    type\n",
        "    \"\"\"\n",
        "    fcols = df.select_dtypes('float').columns\n",
        "\n",
        "    icols = df.select_dtypes('integer').columns\n",
        "\n",
        "    df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\n",
        "\n",
        "    df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "fdn_rRJa81fR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c305571a-fcff-40ba-c6b3-2eafab15c504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset.py"
      ],
      "metadata": {
        "id": "pCXSA5mwDDcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "\n",
        "class TransformerDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class used for transformer models.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "        data: torch.tensor,\n",
        "        indices: list,\n",
        "        enc_seq_len: int,\n",
        "        dec_seq_len: int,\n",
        "        target_seq_len: int\n",
        "        ) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "\n",
        "            data: tensor, the entire train, validation or test data sequence\n",
        "                        before any slicing. If univariate, data.size() will be\n",
        "                        [number of samples, number of variables]\n",
        "                        where the number of variables will be equal to 1 + the number of\n",
        "                        exogenous variables. Number of exogenous variables would be 0\n",
        "                        if univariate.\n",
        "\n",
        "            indices: a list of tuples. Each tuple has two elements:\n",
        "                     1) the start index of a sub-sequence\n",
        "                     2) the end index of a sub-sequence.\n",
        "                     The sub-sequence is split into src, trg and trg_y later.\n",
        "\n",
        "            enc_seq_len: int, the desired length of the input sequence given to the\n",
        "                     the first layer of the transformer model.\n",
        "\n",
        "            target_seq_len: int, the desired length of the target sequence (the output of the model)\n",
        "\n",
        "            target_idx: The index position of the target variable in data. Data\n",
        "                        is a 2D tensor\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.indices = indices\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        print(\"From get_src_trg: data size = {}\".format(data.size()))\n",
        "\n",
        "        self.enc_seq_len = enc_seq_len\n",
        "\n",
        "        self.dec_seq_len = dec_seq_len\n",
        "\n",
        "        self.target_seq_len = target_seq_len\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns a tuple with 3 elements:\n",
        "        1) src (the encoder input)\n",
        "        2) trg (the decoder input)\n",
        "        3) trg_y (the target)\n",
        "        \"\"\"\n",
        "        # Get the first element of the i'th tuple in the list self.indicesasdfas\n",
        "        start_idx = self.indices[index][0]\n",
        "\n",
        "        # Get the second (and last) element of the i'th tuple in the list self.indices\n",
        "        end_idx = self.indices[index][1]\n",
        "\n",
        "        sequence = self.data[start_idx:end_idx]\n",
        "\n",
        "        #print(\"From __getitem__: sequence length = {}\".format(len(sequence)))\n",
        "\n",
        "        src, trg, trg_y = self.get_src_trg(\n",
        "            sequence=sequence,\n",
        "            enc_seq_len=self.enc_seq_len,\n",
        "            dec_seq_len=self.dec_seq_len,\n",
        "            target_seq_len=self.target_seq_len\n",
        "            )\n",
        "\n",
        "        return src, trg, trg_y\n",
        "\n",
        "    def get_src_trg(\n",
        "        self,\n",
        "        sequence: torch.Tensor,\n",
        "        enc_seq_len: int,\n",
        "        dec_seq_len: int,\n",
        "        target_seq_len: int\n",
        "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
        "\n",
        "        \"\"\"\n",
        "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
        "        sequences from a sequence.\n",
        "\n",
        "        Args:\n",
        "\n",
        "            sequence: tensor, a 1D tensor of length n where\n",
        "                    n = encoder input length + target sequence length\n",
        "\n",
        "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
        "\n",
        "            target_seq_len: int, the desired length of the target sequence (the\n",
        "                            one against which the model output is compared)\n",
        "\n",
        "        Return:\n",
        "\n",
        "            src: tensor, 1D, used as input to the transformer model\n",
        "\n",
        "            trg: tensor, 1D, used as input to the transformer model\n",
        "\n",
        "            trg_y: tensor, 1D, the target sequence against which the model output\n",
        "                is compared when computing loss.\n",
        "\n",
        "        \"\"\"\n",
        "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
        "\n",
        "        # encoder input\n",
        "        src = sequence[:enc_seq_len]\n",
        "\n",
        "        # decoder input. As per the paper, it must have the same dimension as the\n",
        "        # target sequence, and it must contain the last value of src, and all\n",
        "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
        "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
        "\n",
        "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
        "\n",
        "        # The target sequence against which the model output will be compared to compute loss\n",
        "        trg_y = sequence[-target_seq_len:]\n",
        "\n",
        "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
        "\n",
        "        return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len]"
      ],
      "metadata": {
        "id": "foiSILGECwdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inference.py"
      ],
      "metadata": {
        "id": "ib8m7maTC4sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Code for running inference with transformer\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def run_encoder_decoder_inference(\n",
        "    model: nn.Module,\n",
        "    src: torch.Tensor,\n",
        "    forecast_window: int,\n",
        "    batch_size: int,\n",
        "    device,\n",
        "    batch_first: bool=False\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    NB! This function is currently only tested on models that work with\n",
        "    batch_first = False\n",
        "\n",
        "    This function is for encoder-decoder type models in which the decoder requires\n",
        "    an input, tgt, which - during training - is the target sequence. During inference,\n",
        "    the values of tgt are unknown, and the values therefore have to be generated\n",
        "    iteratively.\n",
        "\n",
        "    This function returns a prediction of length forecast_window for each batch in src\n",
        "\n",
        "    NB! If you want the inference to be done without gradient calculation,\n",
        "    make sure to call this function inside the context manager torch.no_grad like:\n",
        "    with torch.no_grad:\n",
        "        run_encoder_decoder_inference()\n",
        "\n",
        "    The context manager is intentionally not called inside this function to make\n",
        "    it usable in cases where the function is used to compute loss that must be\n",
        "    backpropagated during training and gradient calculation hence is required.\n",
        "\n",
        "    If use_predicted_tgt = True:\n",
        "    To begin with, tgt is equal to the last value of src. Then, the last element\n",
        "    in the model's prediction is iteratively concatenated with tgt, such that\n",
        "    at each step in the for-loop, tgt's size increases by 1. Finally, tgt will\n",
        "    have the correct length (target sequence length) and the final prediction\n",
        "    will be produced and returned.\n",
        "\n",
        "    Args:\n",
        "        model: An encoder-decoder type model where the decoder requires\n",
        "               target values as input. Should be set to evaluation mode before\n",
        "               passed to this function.\n",
        "\n",
        "        src: The input to the model\n",
        "\n",
        "        forecast_horizon: The desired length of the model's output, e.g. 58 if you\n",
        "                         want to predict the next 58 hours of FCR prices.\n",
        "\n",
        "        batch_size: batch size\n",
        "\n",
        "        batch_first: If true, the shape of the model input should be\n",
        "                     [batch size, input sequence length, number of features].\n",
        "                     If false, [input sequence length, batch size, number of features]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Dimension of a batched model input that contains the target sequence values\n",
        "    target_seq_dim = 0 if batch_first == False else 1\n",
        "\n",
        "    # Take the last value of thetarget variable in all batches in src and make it tgt\n",
        "    # as per the Influenza paper\n",
        "    tgt = src[-1, :, 0] if batch_first == False else src[:, -1, 0] # shape [1, batch_size, 1]\n",
        "\n",
        "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
        "    if batch_size == 1 and batch_first == False:\n",
        "        tgt = tgt.unsqueeze(0).unsqueeze(0) # change from [1] to [1, 1, 1]\n",
        "\n",
        "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
        "    if batch_first == False and batch_size > 1:\n",
        "        tgt = tgt.unsqueeze(0).unsqueeze(-1)\n",
        "\n",
        "    # Iteratively concatenate tgt with the first element in the prediction\n",
        "    for _ in range(forecast_window-1):\n",
        "\n",
        "        # Create masks\n",
        "        dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
        "\n",
        "        dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
        "\n",
        "        tgt_mask = generate_square_subsequent_mask(\n",
        "            dim1=dim_a,\n",
        "            dim2=dim_a,\n",
        "            device=device\n",
        "            )\n",
        "\n",
        "        src_mask = generate_square_subsequent_mask(\n",
        "            dim1=dim_a,\n",
        "            dim2=dim_b,\n",
        "            device=device\n",
        "            )\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "        # If statement simply makes sure that the predicted value is\n",
        "        # extracted and reshaped correctly\n",
        "        if batch_first == False:\n",
        "\n",
        "            # Obtain the predicted value at t+1 where t is the last time step\n",
        "            # represented in tgt\n",
        "            last_predicted_value = prediction[-1, :, :]\n",
        "\n",
        "            # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
        "            last_predicted_value = last_predicted_value.unsqueeze(0)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Obtain predicted value\n",
        "            last_predicted_value = prediction[:, -1, :]\n",
        "\n",
        "            # Reshape from [batch_size, 1] --> [batch_size, 1, 1]\n",
        "            last_predicted_value = last_predicted_value.unsqueeze(-1)\n",
        "\n",
        "        # Detach the predicted element from the graph and concatenate with\n",
        "        # tgt in dimension 1 or 0\n",
        "        tgt = torch.cat((tgt, last_predicted_value.detach()), target_seq_dim)\n",
        "\n",
        "    # Create masks\n",
        "    dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
        "    dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(\n",
        "        dim1=dim_a,\n",
        "        dim2=dim_a,\n",
        "        device=device\n",
        "        )\n",
        "\n",
        "    src_mask = generate_square_subsequent_mask(\n",
        "        dim1=dim_a,\n",
        "        dim2=dim_b,\n",
        "        device=device\n",
        "        )\n",
        "\n",
        "    # Make final prediction\n",
        "    final_prediction = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "    return final_prediction"
      ],
      "metadata": {
        "id": "kC59eyH7CwaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inference_example.py"
      ],
      "metadata": {
        "id": "SrBd0zzVCw0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# Pseudo code-ish example of how to use the inference function to do validation\n",
        "# during training.\n",
        "\n",
        "# The validation loop can be used as-is for model testing as well.\n",
        "\n",
        "# NB! You cannot use this script as is. This is merely an example to show the overall idea -\n",
        "# not something you can copy paste and expect to work. For instance, see \"sandbox.py\"\n",
        "# for example of how to instantiate model and generate dataloaders.\n",
        "\n",
        "# If you have never before trained a PyTorch neural network, I suggest you look\n",
        "# at some of PyTorch's beginner-level tutorials.\n",
        "# \"\"\"\n",
        "# import torch\n",
        "# # import inference\n",
        "# # import utils\n",
        "\n",
        "# epochs = 10\n",
        "\n",
        "# forecast_window = 48 # supposing you're forecasting 48 hours ahead\n",
        "\n",
        "# enc_seq_len = 168 # supposing you want the model to base its forecasts on the previous 7 days of data\n",
        "\n",
        "# optimizer = torch.optim.Adam()\n",
        "\n",
        "# criterion = torch.nn.MSELoss()\n",
        "\n",
        "# # Iterate over all epochs\n",
        "# for epoch in range(epochs):\n",
        "\n",
        "#     # Iterate over all (x,y) pairs in training dataloader\n",
        "#     for i, (src, tgt, tgt_y) in enumerate(training_dataloader):\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Generate masks\n",
        "#         tgt_mask = utils.generate_square_subsequent_mask(\n",
        "#             dim1=forecast_window,\n",
        "#             dim2=forecast_window\n",
        "#             )\n",
        "\n",
        "#         src_mask = utils.generate_square_subsequent_mask(\n",
        "#             dim1=forecast_window,\n",
        "#             dim2=enc_seq_len\n",
        "#             )\n",
        "\n",
        "#         # Make forecasts\n",
        "#         prediction = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "#         # Compute and backprop loss\n",
        "#         loss = criterion(tgt_y, prediction)\n",
        "\n",
        "#         loss.backward()\n",
        "\n",
        "#         # Take optimizer step\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "#     # Iterate over all (x,y) pairs in validation dataloader\n",
        "#     model.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "\n",
        "#         for i, (src, _, tgt_y) in enumerate(validation_dataloader):\n",
        "\n",
        "#             prediction = run_encoder_decoder_inference(\n",
        "#                 model=model,\n",
        "#                 src=src,\n",
        "#                 forecast_window=forecast_window,\n",
        "#                 batch_size=src.shape[1]\n",
        "#                 )\n",
        "\n",
        "#             loss = criterion(tgt_y, prediction)"
      ],
      "metadata": {
        "id": "Fg1HWosRCwBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "positional_encoder.py"
      ],
      "metadata": {
        "id": "-2agC8JdCpZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch import nn, Tensor\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The authors of the original transformer paper describe very succinctly what\n",
        "    the positional encoding layer does and why it is needed:\n",
        "\n",
        "    \"Since our model contains no recurrence and no convolution, in order for the\n",
        "    model to make use of the order of the sequence, we must inject some\n",
        "    information about the relative or absolute position of the tokens in the\n",
        "    sequence.\" (Vaswani et al, 2017)\n",
        "    Adapted from:\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dropout: float=0.1,\n",
        "        max_seq_len: int=5000,\n",
        "        d_model: int=512,\n",
        "        batch_first: bool=False\n",
        "        ):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            dropout: the dropout rate\n",
        "            max_seq_len: the maximum length of the input sequences\n",
        "            d_model: The dimension of the output of sub-layers in the model\n",
        "                     (Vaswani et al, 2017)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        # adapted from PyTorch tutorial\n",
        "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        if self.batch_first:\n",
        "            pe = torch.zeros(1, max_seq_len, d_model)\n",
        "\n",
        "            pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "            pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        else:\n",
        "            pe = torch.zeros(max_seq_len, 1, d_model)\n",
        "\n",
        "            pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or\n",
        "               [enc_seq_len, batch_size, dim_val]\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            x = x + self.pe[:,:x.size(1)]\n",
        "        else:\n",
        "            x = x + self.pe[:x.size(0)]\n",
        "\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "1o_UV_iWqOtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer_timeseries.py"
      ],
      "metadata": {
        "id": "xx3Iu7-IChQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import nn, Tensor\n",
        "# import positional_encoder as pe\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    This class implements a transformer model that can be used for times series\n",
        "    forecasting. This time series transformer model is based on the paper by\n",
        "    Wu et al (2020) [1]. The paper will be referred to as \"the paper\".\n",
        "\n",
        "    A detailed description of the code can be found in my article here:\n",
        "\n",
        "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
        "\n",
        "    In cases where the paper does not specify what value was used for a specific\n",
        "    configuration/hyperparameter, this class uses the values from Vaswani et al\n",
        "    (2017) [2] or from PyTorch source code.\n",
        "\n",
        "    Unlike the paper, this class assumes that input layers, positional encoding\n",
        "    layers and linear mapping layers are separate from the encoder and decoder,\n",
        "    i.e. the encoder and decoder only do what is depicted as their sub-layers\n",
        "    in the paper. For practical purposes, this assumption does not make a\n",
        "    difference - it merely means that the linear and positional encoding layers\n",
        "    are implemented inside the present class and not inside the\n",
        "    Encoder() and Decoder() classes.\n",
        "\n",
        "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020).\n",
        "    'Deep Transformer Models for Time Series Forecasting:\n",
        "    The Influenza Prevalence Case'.\n",
        "    arXiv:2001.08317 [cs, stat] [Preprint].\n",
        "    Available at: http://arxiv.org/abs/2001.08317 (Accessed: 9 March 2022).\n",
        "\n",
        "    [2] Vaswani, A. et al. (2017)\n",
        "    'Attention Is All You Need'.\n",
        "    arXiv:1706.03762 [cs] [Preprint].\n",
        "    Available at: http://arxiv.org/abs/1706.03762 (Accessed: 9 March 2022).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        input_size: int,\n",
        "        dec_seq_len: int,\n",
        "        batch_first: bool,\n",
        "        out_seq_len: int=58,\n",
        "        dim_val: int=512,\n",
        "        n_encoder_layers: int=4,\n",
        "        n_decoder_layers: int=4,\n",
        "        n_heads: int=8,\n",
        "        dropout_encoder: float=0.2,\n",
        "        dropout_decoder: float=0.2,\n",
        "        dropout_pos_enc: float=0.1,\n",
        "        dim_feedforward_encoder: int=2048,\n",
        "        dim_feedforward_decoder: int=2048,\n",
        "        num_predicted_features: int=1\n",
        "        ):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "\n",
        "            input_size: int, number of input variables. 1 if univariate.\n",
        "\n",
        "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
        "\n",
        "            dim_val: int, aka d_model. All sub-layers in the model produce\n",
        "                     outputs of dimension dim_val\n",
        "\n",
        "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
        "\n",
        "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
        "\n",
        "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
        "\n",
        "            dropout_encoder: float, the dropout rate of the encoder\n",
        "\n",
        "            dropout_decoder: float, the dropout rate of the decoder\n",
        "\n",
        "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
        "\n",
        "            dim_feedforward_encoder: int, number of neurons in the linear layer\n",
        "                                     of the encoder\n",
        "\n",
        "            dim_feedforward_decoder: int, number of neurons in the linear layer\n",
        "                                     of the decoder\n",
        "\n",
        "            num_predicted_features: int, the number of features you want to predict.\n",
        "                                    Most of the time, this will be 1 because we're\n",
        "                                    only forecasting FCR-N prices in DK2, but in\n",
        "                                    we wanted to also predict FCR-D with the same\n",
        "                                    model, num_predicted_features should be 2.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.dec_seq_len = dec_seq_len\n",
        "\n",
        "        #print(\"input_size is: {}\".format(input_size))\n",
        "        #print(\"dim_val is: {}\".format(dim_val))\n",
        "\n",
        "        # Creating the three linear layers needed for the model\n",
        "        self.encoder_input_layer = nn.Linear(\n",
        "            in_features=input_size,\n",
        "            out_features=dim_val\n",
        "            )\n",
        "\n",
        "        self.decoder_input_layer = nn.Linear(\n",
        "            in_features=num_predicted_features,\n",
        "            out_features=dim_val\n",
        "            )\n",
        "\n",
        "        self.linear_mapping = nn.Linear(\n",
        "            in_features=dim_val,\n",
        "            out_features=num_predicted_features\n",
        "            )\n",
        "\n",
        "        # Create positional encoder\n",
        "        self.positional_encoding_layer = PositionalEncoder(\n",
        "            d_model=dim_val,\n",
        "            dropout=dropout_pos_enc\n",
        "            )\n",
        "\n",
        "        # The encoder layer used in the paper is identical to the one used by\n",
        "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=dim_val,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward_encoder,\n",
        "            dropout=dropout_encoder,\n",
        "            batch_first=batch_first\n",
        "            )\n",
        "\n",
        "        # Stack the encoder layers in nn.TransformerDecoder\n",
        "        # It seems the option of passing a normalization instance is redundant\n",
        "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
        "        # after each sub-layer\n",
        "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer=encoder_layer,\n",
        "            num_layers=n_encoder_layers,\n",
        "            norm=None\n",
        "            )\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=dim_val,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward_decoder,\n",
        "            dropout=dropout_decoder,\n",
        "            batch_first=batch_first\n",
        "            )\n",
        "\n",
        "        # Stack the decoder layers in nn.TransformerDecoder\n",
        "        # It seems the option of passing a normalization instance is redundant\n",
        "        # in my case, because nn.TransformerDecoderLayer per default normalizes\n",
        "        # after each sub-layer\n",
        "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            decoder_layer=decoder_layer,\n",
        "            num_layers=n_decoder_layers,\n",
        "            norm=None\n",
        "            )\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None,\n",
        "                tgt_mask: Tensor=None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Returns a tensor of shape:\n",
        "\n",
        "        [target_sequence_length, batch_size, num_predicted_features]\n",
        "\n",
        "        Args:\n",
        "\n",
        "            src: the encoder's output sequence. Shape: (S,E) for unbatched input,\n",
        "                 (S, N, E) if batch_first=False or (N, S, E) if\n",
        "                 batch_first=True, where S is the source sequence length,\n",
        "                 N is the batch size, and E is the number of features (1 if univariate)\n",
        "\n",
        "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input,\n",
        "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if\n",
        "                 batch_first=True, where T is the target sequence length,\n",
        "                 N is the batch size, and E is the number of features (1 if univariate)\n",
        "\n",
        "            src_mask: the mask for the src sequence to prevent the model from\n",
        "                      using data points from the target sequence\n",
        "\n",
        "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
        "                      using data points from the target sequence\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
        "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
        "\n",
        "        # Pass throguh the input layer right before the encoder\n",
        "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
        "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
        "\n",
        "        # Pass through the positional encoding layer\n",
        "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
        "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
        "\n",
        "        # Pass through all the stacked encoder layers in the encoder\n",
        "        # Masking is only needed in the encoder if input sequences are padded\n",
        "        # which they are not in this time series use case, because all my\n",
        "        # input sequences are naturally of the same length.\n",
        "        # (https://github.com/huggingface/transformers/issues/4083)\n",
        "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
        "            src=src\n",
        "            )\n",
        "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
        "\n",
        "        # Pass decoder input through decoder input layer\n",
        "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
        "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
        "\n",
        "        #if src_mask is not None:\n",
        "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
        "        #if tgt_mask is not None:\n",
        "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
        "\n",
        "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
        "        decoder_output = self.decoder(\n",
        "            tgt=decoder_output,\n",
        "            memory=src,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=src_mask\n",
        "            )\n",
        "\n",
        "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
        "\n",
        "        # Pass through linear mapping\n",
        "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
        "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
        "\n",
        "        return decoder_output"
      ],
      "metadata": {
        "id": "kbztfJ3681iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sandbox.py"
      ],
      "metadata": {
        "id": "J8HrWHYRCnWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Showing how to use the model with some time series data.\n",
        "\n",
        "NB! This is not a full training loop. You have to write the training loop yourself.\n",
        "\n",
        "I.e. this code is just a starting point to show you how to initialize the model and provide its inputs\n",
        "\n",
        "If you do not know how to train a PyTorch model, it is too soon for you to dive into transformers imo :)\n",
        "\n",
        "You're better off starting off with some simpler architectures, e.g. a simple feed forward network, in order to learn the basics\n",
        "\"\"\"\n",
        "\n",
        "# import dataset as ds\n",
        "# import utils\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import datetime\n",
        "# import transformer_timeseries as tst\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Hyperparams\n",
        "test_size = 0.1\n",
        "batch_size = 128\n",
        "target_col_name = \"FCR_N_PriceEUR\"\n",
        "timestamp_col = \"timestamp\"\n",
        "# Only use data from this date and onwards\n",
        "cutoff_date = datetime.datetime(2017, 1, 1)\n",
        "\n",
        "## Params\n",
        "dim_val = 512\n",
        "n_heads = 8\n",
        "n_decoder_layers = 4\n",
        "n_encoder_layers = 4\n",
        "dec_seq_len = 92 # length of input given to decoder\n",
        "enc_seq_len = 153 # length of input given to encoder\n",
        "output_sequence_length = 48 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
        "window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences\n",
        "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
        "in_features_encoder_linear_layer = 2048\n",
        "in_features_decoder_linear_layer = 2048\n",
        "max_seq_len = enc_seq_len\n",
        "batch_first = False\n",
        "\n",
        "# Define input variables\n",
        "exogenous_vars = [] # should contain strings. Each string must correspond to a column name\n",
        "input_variables = [target_col_name] + exogenous_vars\n",
        "target_idx = 0 # index position of target in batched trg_y\n",
        "\n",
        "input_size = len(input_variables)\n",
        "\n",
        "# Read data\n",
        "data = read_data(timestamp_col_name=timestamp_col)\n",
        "\n",
        "# Remove test data from dataset\n",
        "training_data = data[:-(round(len(data)*test_size))]\n",
        "\n",
        "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chunkc.\n",
        "# Should be training data indices only\n",
        "training_indices = get_indices_entire_sequence(\n",
        "    data=training_data,\n",
        "    window_size=window_size,\n",
        "    step_size=step_size)\n",
        "\n",
        "# Making instance of custom dataset class\n",
        "training_data = TransformerDataset(\n",
        "    data=torch.tensor(training_data[input_variables].values).float(),\n",
        "    indices=training_indices,\n",
        "    enc_seq_len=enc_seq_len,\n",
        "    dec_seq_len=dec_seq_len,\n",
        "    target_seq_len=output_sequence_length\n",
        "    )\n",
        "\n",
        "# Making dataloader\n",
        "training_data = DataLoader(training_data, batch_size)\n",
        "\n",
        "i, batch = next(enumerate(training_data))\n",
        "\n",
        "src, trg, trg_y = batch\n",
        "\n",
        "# Permute from shape [batch size, seq len, num features] to [seq len, batch size, num features]\n",
        "if batch_first == False:\n",
        "\n",
        "    shape_before = src.shape\n",
        "    src = src.permute(1, 0, 2)\n",
        "    print(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
        "\n",
        "    shape_before = trg.shape\n",
        "    trg = trg.permute(1, 0, 2)\n",
        "    print(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
        "\n",
        "model = TimeSeriesTransformer(\n",
        "    input_size=len(input_variables),\n",
        "    dec_seq_len=enc_seq_len,\n",
        "    batch_first=batch_first,\n",
        "    num_predicted_features=1\n",
        "    )\n",
        "\n",
        "# Make src mask for decoder with size:\n",
        "# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
        "src_mask = generate_square_subsequent_mask(\n",
        "    dim1=output_sequence_length,\n",
        "    dim2=enc_seq_len\n",
        "    )\n",
        "\n",
        "# Make tgt mask for decoder with size:\n",
        "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
        "tgt_mask = generate_squ are_subsequent_mask(\n",
        "    dim1=output_sequence_length,\n",
        "    dim2=output_sequence_length\n",
        "    )\n",
        "\n",
        "output = model(\n",
        "    src=src,\n",
        "    tgt=trg,\n",
        "    src_mask=src_mask,\n",
        "    tgt_mask=tgt_mask\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2urd4uQqQMB",
        "outputId": "2550a52e-3b78-460f-d6fa-3ab6146af4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file in /content/drive/MyDrive/240705/medium/dfs_merged_upload.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b541352b3b95>:170: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  data = pd.read_csv(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From get_src_trg: data size = torch.Size([41387, 1])\n",
            "src shape changed from torch.Size([128, 153, 1]) to torch.Size([153, 128, 1])\n",
            "src shape changed from torch.Size([128, 48, 1]) to torch.Size([153, 128, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDwT0Dos81cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyxqR3Ql81Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoDl96a_81XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JALGpVu_81U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbJPXb8P81Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vx2l3F1j81PQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}